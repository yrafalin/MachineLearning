{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqDNVAuZa03l"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "1. Go to https://colab.research.google.com and choose the \\\"Upload\\\" option to upload this notebook file.\n",
    "1. In the Edit menu, choose \\\"Notebook Settings\\\" and then set the \\\"Hardware Accelerator\\\" dropdown to GPU.\n",
    "1. Read through the code in the following sections:\n",
    "  * [IMDB Dataset](#scrollTo=mXcb24B6a03_)\n",
    "  * [Define model](#scrollTo=kAz68ipVa05_)\n",
    "  * [Train model](#scrollTo=kIynp1v_a06Y)\n",
    "  * [Assess model](#scrollTo=ALyNCqx4a06r)\n",
    "1. Complete at least one of these exercises. Remember to keep notes about what you do!\n",
    "  * [Exercise Option #1 - Standard Difficulty](#scrollTo=_9dsjJwya06_)\n",
    "  * [Exercise Option #2 - Advanced Difficulty](#scrollTo=nyZbljLAa09z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POc2jClCa03y"
   },
   "source": [
    "## Documentation/Sources\n",
    "* [Class Notes](https://jennselby.github.io/MachineLearningCourseNotes/#recurrent-neural-networks)\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation\n",
    "* [Keras recurrent tutorial](https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h04E5miUb8wh"
   },
   "outputs": [],
   "source": [
    "# upgrade tensorflow to tensorflow 2\n",
    "%tensorflow_version 2.x\n",
    "# display matplotlib plots\n",
    "%matplotlib inline\n",
    "from tensorflow import test\n",
    "from tensorflow import device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXcb24B6a03_"
   },
   "source": [
    "# IMDB Dataset\n",
    "The [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) consists of movie reviews (x_train) that have been marked as positive or negative (y_train). See the [Word Vectors Tutorial](https://github.com/jennselby/MachineLearningTutorials/blob/master/WordVectors.ipynb) for more details on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u2kXcDIia04D"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWuzcUyua04f",
    "outputId": "1184cd4c-3280-4655-87a5-7b5cee28e84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(imdb_x_train, imdb_y_train), (imdb_x_test, imdb_y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYQ3yPO4a04x"
   },
   "source": [
    "For a standard keras model, every input has to be the same length, so we need to set some length after which we will cutoff the rest of the review. (We will also need to pad the shorter reviews with zeros to make them the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QtYp3G31a040"
   },
   "outputs": [],
   "source": [
    "cutoff = 500\n",
    "imdb_x_train_padded = sequence.pad_sequences(imdb_x_train, maxlen=cutoff)\n",
    "imdb_x_test_padded = sequence.pad_sequences(imdb_x_test, maxlen=cutoff)\n",
    "\n",
    " # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "imdb_index_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iOGRpn25a05o"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAz68ipVa05_"
   },
   "source": [
    "# Define model\n",
    "\n",
    "Unlike last time, when we used convolutional layers, we're going to use an LSTM, a special type of recurrent network.\n",
    "\n",
    "Using recurrent networks means that rather than seeing these reviews as one input happening all at once, with the convolutional layers taking into account which words are next to each other, we are going to see them as a sequence of inputs, with one word occurring at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u9ZArrxa06G",
    "outputId": "1f20dfa2-2c15-4672-cdeb-a8a5e624bd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb_lstm_model = Sequential()\n",
    "imdb_lstm_model.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
    "                              output_dim=100,\n",
    "                              input_length=cutoff))\n",
    "# return_sequences tells the LSTM to output the full sequence, for use by the next LSTM layer. The final\n",
    "# LSTM layer should return only the output sequence, for use in the Dense output layer\n",
    "imdb_lstm_model.add(LSTM(units=32, return_sequences=True))\n",
    "imdb_lstm_model.add(LSTM(units=32))\n",
    "imdb_lstm_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "imdb_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIynp1v_a06Y"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ktpTQmpa06Z",
    "outputId": "c94e6e6c-b103-4ee5-b55b-a863cc88d5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 86s 134ms/step - loss: 0.4993 - binary_accuracy: 0.7329\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU acceleration\n",
    "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  imdb_lstm_model.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALyNCqx4a06r"
   },
   "source": [
    "# Assess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzNKy7fCa06y",
    "outputId": "02e257cf-9326-4619-fad7-ed4cfa4b3677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3067 - binary_accuracy: 0.8713\n",
      "loss: 0.3066539764404297 accuracy: 0.8712800145149231\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  imdb_lstm_scores = imdb_lstm_model.evaluate(imdb_x_test_padded, imdb_y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*imdb_lstm_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9dsjJwya06_"
   },
   "source": [
    "# Exercise Option #1 - Standard Difficulty\n",
    "\n",
    "Experiment with different model configurations from the one above. Try other recurrent layers, different numbers of layers, change some of the defaults. See [Keras Recurrent Layers](https://keras.io/layers/recurrent/)\n",
    "\n",
    "__Keep notes on what you try and what results you get.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8lbjGNeuQV6"
   },
   "source": [
    "87% is already a high bar to beat, but I'll begin by trying adding another LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mWGqX9LBpki5"
   },
   "outputs": [],
   "source": [
    "imdb_model2 = Sequential()\n",
    "imdb_model2.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
    "                              output_dim=100,\n",
    "                              input_length=cutoff))\n",
    "imdb_model2.add(LSTM(units=32, return_sequences=True))\n",
    "imdb_model2.add(LSTM(units=32, return_sequences=True))\n",
    "imdb_model2.add(LSTM(units=32))\n",
    "imdb_model2.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "imdb_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ds6cKoTopki_",
    "outputId": "e8e5301a-b08a-4f3e-f980-4cea22810e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 59s 141ms/step - loss: 0.4993 - binary_accuracy: 0.7283\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU acceleration\n",
    "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  imdb_model2.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hNsSq_8pkjA",
    "outputId": "4b2d0f9a-39d0-431b-befb-0cb207b9b51a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 18s 21ms/step - loss: 0.3453 - binary_accuracy: 0.8535\n",
      "loss: 0.3453221023082733 accuracy: 0.8535199761390686\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  imdb_scores2 = imdb_model2.evaluate(imdb_x_test_padded, imdb_y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*imdb_scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQvCuj8HukwZ"
   },
   "source": [
    "This seems to give around the same accuracy, if not slightly worse. For the next model I tried adding a dense layer after the LSTM layers to see if they might help filtering the results of the LSTM better than a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OC40z9zKqV_r"
   },
   "outputs": [],
   "source": [
    "imdb_model3 = Sequential()\n",
    "imdb_model3.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
    "                              output_dim=100,\n",
    "                              input_length=cutoff))\n",
    "imdb_model3.add(LSTM(units=32, return_sequences=True))\n",
    "imdb_model3.add(LSTM(units=32))\n",
    "imdb_model3.add(Dense(units=64))\n",
    "imdb_model3.add(Dense(units=1, activation='sigmoid'))\n",
    "imdb_model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0PwD8iCqV_t",
    "outputId": "363a5fc8-c0a3-4c34-fc71-8aec2a5e6672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 56s 137ms/step - loss: 0.4906 - binary_accuracy: 0.7458\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU acceleration\n",
    "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  imdb_model3.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfH8NrEDqV_u",
    "outputId": "c66da6dc-bc26-4525-e2fc-75247c54f825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 16ms/step - loss: 0.3085 - binary_accuracy: 0.8751\n",
      "loss: 0.3084729015827179 accuracy: 0.8751199841499329\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  imdb_scores3 = imdb_model3.evaluate(imdb_x_test_padded, imdb_y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*imdb_scores3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE0-myWGumZK"
   },
   "source": [
    "This gave a very similar accuracy. Then I tried making a wider model, to add more LSTM units, thinking that giving more units might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n1I76Hocr9lo"
   },
   "outputs": [],
   "source": [
    "imdb_model4 = Sequential()\n",
    "imdb_model4.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
    "                              output_dim=100,\n",
    "                              input_length=cutoff))\n",
    "imdb_model4.add(LSTM(units=64, return_sequences=True))\n",
    "imdb_model4.add(LSTM(units=64))\n",
    "imdb_model4.add(Dense(units=1, activation='sigmoid'))\n",
    "imdb_model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baKbnRVmr9lu",
    "outputId": "c72f46de-2221-47f1-83ec-e24967186e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 55s 135ms/step - loss: 0.5053 - binary_accuracy: 0.7231\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU acceleration\n",
    "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  imdb_model4.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97SNvBnbr9lv",
    "outputId": "0d7b5047-02e2-41fe-9ecb-2dbb2be44896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 16ms/step - loss: 0.3414 - binary_accuracy: 0.8624\n",
      "loss: 0.3413776457309723 accuracy: 0.8623999953269958\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  imdb_scores4 = imdb_model4.evaluate(imdb_x_test_padded, imdb_y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*imdb_scores4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeVXw3Zhu8-I"
   },
   "source": [
    "This ended up not helping much either. Lastly, I thought the model might just be overcomplicating, so I lowered the unit number and added a small dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lu7D9XfCssOX"
   },
   "outputs": [],
   "source": [
    "imdb_model5 = Sequential()\n",
    "imdb_model5.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
    "                              output_dim=100,\n",
    "                              input_length=cutoff))\n",
    "imdb_model5.add(LSTM(units=16, return_sequences=True))\n",
    "imdb_model5.add(LSTM(units=16))\n",
    "imdb_model3.add(Dense(units=16))\n",
    "imdb_model5.add(Dense(units=1, activation='sigmoid'))\n",
    "imdb_model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fi12mOBjssOj",
    "outputId": "1e4ffd52-ed4b-47d1-9e6f-855ede9e4999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 54s 131ms/step - loss: 0.5233 - binary_accuracy: 0.7153\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU acceleration\n",
    "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  imdb_model5.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCUjOhMzssOk",
    "outputId": "13bc706d-3cdc-49be-bd9e-45737624ce58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 16ms/step - loss: 0.3123 - binary_accuracy: 0.8696\n",
      "loss: 0.3122839331626892 accuracy: 0.8696399927139282\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  imdb_scores5 = imdb_model5.evaluate(imdb_x_test_padded, imdb_y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*imdb_scores5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLMfpr7pt2vp"
   },
   "source": [
    "Again, this also did not improve the model much. I've tried a lot of changes but each one has had minimal effect on the accuracy of the model, which makes me think that either this dataset has somewhat of an upper bound for accuracy, at least for LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vWAk0K-ynyi",
    "outputId": "d2fcbeb7-bf56-4ee5-d3c6-0a81b2014609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 100)          8858700   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 500, 16)           7488      \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 8,868,317\n",
      "Trainable params: 8,868,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyZbljLAa09z"
   },
   "source": [
    "# Exercise Option #2 - Advanced Difficulty\n",
    "\n",
    "Set up your own RNN model for the Reuters Classification Problem\n",
    "\n",
    "Take the model from exercise 1 (imdb_lstm_model) and modify it to classify the [Reuters data](https://keras.io/datasets/#reuters-newswire-topics-classification).\n",
    "\n",
    "Think about what you are trying to predict in this case, and how you will have to change your model to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zI9p998Ra090"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OR-J3MWa095",
    "outputId": "a9eef53c-0c95-4c9f-a833-80360acce01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(reuters_x_train, reuters_y_train), (reuters_x_test, reuters_y_test) = reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MiW4Vhgpa098"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uYmb0IySa0-B"
   },
   "outputs": [],
   "source": [
    "y_onehot_train = to_categorical(reuters_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfLwSrzcwXGK",
    "outputId": "70cdc76b-f8a1-407c-b3a1-e8fc61f41617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb_offset = 3\n",
    "reuters_map = dict((index + imdb_offset, word) for (word, index) in reuters.get_word_index().items())\n",
    "reuters_map[0] = 'PADDING'\n",
    "reuters_map[1] = 'START'\n",
    "reuters_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "a0-RVtuVwbJu",
    "outputId": "fdbf4f36-870e-4baa-e7cb-6205ed190da2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'START mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([reuters_map[word_index] for word_index in reuters_x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4m4Jq-O4xH8Z"
   },
   "outputs": [],
   "source": [
    "x_train_padded = sequence.pad_sequences(reuters_x_train, maxlen=cutoff)\n",
    "x_test_padded = sequence.pad_sequences(reuters_x_test, maxlen=cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NGtGfMjUxlIB"
   },
   "outputs": [],
   "source": [
    "reuters_model = Sequential()\n",
    "reuters_model.add(Embedding(input_dim=len(reuters_map), output_dim=100, input_length=cutoff))\n",
    "reuters_model.add(LSTM(units=32, return_sequences=True))\n",
    "reuters_model.add(LSTM(units=32))\n",
    "reuters_model.add(Dense(units=64, activation='relu'))\n",
    "reuters_model.add(Dense(units=46, activation='softmax'))\n",
    "reuters_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaCVq_wbyGcg",
    "outputId": "6e2be909-5ee2-4430-a78a-5001ecc6ba0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 11s 65ms/step - loss: 0.3777 - binary_accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  reuters_model.fit(x_train_padded, y_onehot_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZT9gwCXNyQab"
   },
   "outputs": [],
   "source": [
    "y_onehot_test = to_categorical(reuters_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7y4odFqyU3J",
    "outputId": "536bd9e2-448d-409e-e4ec-fd76099d4c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 2s 18ms/step - loss: 0.0723 - binary_accuracy: 0.9783\n",
      "loss: 0.07233385741710663 accuracy: 0.9782605171203613\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  reuters_scores = reuters_model.evaluate(x_test_padded, y_onehot_test)\n",
    "print('loss: {} accuracy: {}'.format(*reuters_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AkyyIhbyi8P",
    "outputId": "b57284b0-da66-4716-c506-0876a09df7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 100)          3098200   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 500, 32)           17024     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 3,128,646\n",
      "Trainable params: 3,128,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reuters_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is nearly exactly the same accuracy as what I got for the CNN word embedding models in the previous project (0.9782605171203613), and a high one, I wonder if maybe there's a strict upper bound to how accurate the algorithm can be in classifying these reuters articles, and whether we've hit that."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RecurrentNeuralNetworksTuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
