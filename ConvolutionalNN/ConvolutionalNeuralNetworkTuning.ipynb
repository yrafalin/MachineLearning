{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Read through the code in the following sections:\n",
    "    * [MNIST Data](#MNIST-Data)\n",
    "    * [Convolutional Neural Network Model](#Convolutional-Neural-Network-Model)\n",
    "    * [Train Model](#Train-Model)\n",
    "    * [Validation](#Validation)\n",
    "0. Complete the [Exercise](#Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow matplotlib graphics to display in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# import numpy, for image dimension manipulation\n",
    "import numpy\n",
    "\n",
    "# import validation methods from scikit-learn\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# import the dataset and neural network layers from keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "[MNIST](https://en.wikipedia.org/wiki/MNIST_database) is a famous dataset of images of handwritten numbers. The goal is to be able to figure out which number is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants to describe the MNIST images\n",
    "NUM_ROWS = 28 \n",
    "NUM_COLUMNS = 28\n",
    "NUM_COLORS = 1\n",
    "IMG_SHAPE = (NUM_ROWS, NUM_COLUMNS, NUM_COLORS)\n",
    "\n",
    "# constant to describe the MNIST output labels\n",
    "# there are ten different numbers, 0-9\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "((images_train, labels_train), (images_test, labels_test)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one particular image and its label, to better understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14bebde50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(images_train[17], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14ece3610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(images_train[210], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x152291dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(images_train[211], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14f8aff70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(labels_train[213])\n",
    "matplotlib.pyplot.imshow(images_train[213], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 3 is out of bounds for array of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d0f0e5715ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m213\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0mout_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_axis_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_ndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0mshape_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mnormalize_axis_tuple\u001b[0;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 3 is out of bounds for array of dimension 3"
     ]
    }
   ],
   "source": [
    "model.predict(numpy.expand_dims(images_train[213], axis=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[12345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.8126\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14fab6af0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_train = numpy.expand_dims(images_train, axis=3)\n",
    "\n",
    "# the labels need to be one-hot encoded, to match the ten outputs of our model\n",
    "labels_onehot_train = to_categorical(labels_train)\n",
    "\n",
    "# set up the model to be ready for training\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(images_3d_train, labels_onehot_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6764\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7094\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7529\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.8133\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.0051A: 0s - loss: 2.0\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14eb59880>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(images_3d_train, labels_onehot_train, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_test = numpy.expand_dims(images_test, axis=3)\n",
    "\n",
    "# get the predictions from the model\n",
    "predictions_test_onehot = model.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test = numpy.argmax(predictions_test_onehot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.216"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.241033  , 0.        , 0.29274005, 0.65789474, 0.22222222,\n",
       "        0.        , 0.04878049, 0.17734959, 0.        , 0.58333333]),\n",
       " array([0.85714286, 0.        , 0.36337209, 0.02475248, 0.00203666,\n",
       "        0.        , 0.00208768, 0.87743191, 0.        , 0.01387512]),\n",
       " array([0.3762598 , 0.        , 0.32425422, 0.04770992, 0.00403633,\n",
       "        0.        , 0.004004  , 0.29506052, 0.        , 0.02710552]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest?\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Can you get the model to perform better? Try adding more layers, or taking them away. \n",
    "Take a look at the documentation for the [convolutional](https://keras.io/layers/convolutional/) and [dense](https://keras.io/layers/core/) layers and the [sequential model](https://keras.io/models/sequential/) to understand the options that you have and try out different things.\n",
    "\n",
    "It might also be a good idea to find examples posted online of networks that did well with MNIST and try out some of the configuration they used. Make sure you cite any sources you use!\n",
    "\n",
    "Take notes of what performance you get from different configurations.\n",
    "\n",
    "Comment on what patterns you observed in terms of what changes helped your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model I tried adding more filters, more layers, and more units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=32, kernel_size=3, strides=2, padding='same'))\n",
    "model2.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same'))\n",
    "model2.add(Conv2D(filters=8, kernel_size=2, strides=1, padding='same'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units=20, activation='tanh'))\n",
    "model2.add(Dense(units=20, activation='tanh'))\n",
    "model2.add(Dense(units=20, activation='tanh'))\n",
    "model2.add(Dense(units=20, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model2.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0844\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0753\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0671\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0598\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0532\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0478\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0433\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0393\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0359\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f712d30>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer=SGD(), loss='mean_squared_error')\n",
    "\n",
    "# fit the model to the training data\n",
    "model2.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot2 = model2.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test2 = numpy.argmax(predictions_test_onehot2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8117"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=labels_test, y_pred=predictions_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.80120482, 0.94214162, 0.86419753, 0.64868805, 0.75421473,\n",
       "        0.92253521, 0.89017951, 0.86239397, 0.75235405, 0.82948847]),\n",
       " array([0.95      , 0.96123348, 0.81395349, 0.88118812, 0.86558045,\n",
       "        0.14686099, 0.87995825, 0.89007782, 0.82032854, 0.81962339]),\n",
       " array([0.86928105, 0.9515918 , 0.83832335, 0.7472712 , 0.80606923,\n",
       "        0.25338491, 0.88503937, 0.87601723, 0.7848723 , 0.82452642]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest?\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 7, 16)          4624      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 8)           520       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                7860      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 14,794\n",
      "Trainable params: 14,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decent accuracy from the new model but not great. In the next model I changed the activation function from mean_squared_error to categorical_crossentropy, and the results were absolutely awful. 13% accuracy. Something is seriously wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model3.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=32, kernel_size=3, strides=2, padding='same'))\n",
    "model3.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same'))\n",
    "model3.add(Conv2D(filters=8, kernel_size=2, strides=1, padding='same'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(units=20, activation='tanh'))\n",
    "model3.add(Dense(units=20, activation='tanh'))\n",
    "model3.add(Dense(units=20, activation='tanh'))\n",
    "model3.add(Dense(units=20, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model3.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.3467\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.7426\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.1039\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 2.2271\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.2225\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.2210\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.2203\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.2193\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.2191\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.2184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150345160>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model3.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot3 = model3.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test3 = numpy.argmax(predictions_test_onehot3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1361"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=labels_test, y_pred=predictions_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.89367816, 0.        , 1.        , 0.        , 0.50793651,\n",
       "        0.        , 0.        , 0.10607009, 0.        , 0.        ]),\n",
       " array([3.17346939e-01, 0.00000000e+00, 9.68992248e-04, 0.00000000e+00,\n",
       "        3.25865580e-02, 0.00000000e+00, 0.00000000e+00, 9.89299611e-01,\n",
       "        0.00000000e+00, 0.00000000e+00]),\n",
       " array([0.46837349, 0.        , 0.00193611, 0.        , 0.06124402,\n",
       "        0.        , 0.        , 0.19159759, 0.        , 0.        ]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest?\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After talking to Brandon\n",
    "He showed me how he got 91% accuracy by changing a few features: strides to 1, adding activation functions for conv layers, and maxpooling. I incorporated shorter strides + activation functions and removed dense layers and got a whopping 98%!!! Also, categorical crossentropy redeemed itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model4.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=32, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "model4.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(units=20, activation='relu'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model4.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2002\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 37s 19ms/step - loss: 0.0715\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0533\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.0428\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0365\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0313\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0267\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0222\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0194\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150f91c40>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "#images_3d_train = numpy.expand_dims(images_train, axis=3)\n",
    "\n",
    "# the labels need to be one-hot encoded, to match the ten outputs of our model\n",
    "#labels_onehot_train = to_categorical(labels_train)\n",
    "\n",
    "# set up the model to be ready for training\n",
    "model4.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model4.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0137\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0113\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0098\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0081\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0069\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0059\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0053\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0045\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0040\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x153a1d760>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot4 = model4.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test4 = numpy.argmax(predictions_test_onehot4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9871"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.9779338 , 0.99033392, 0.9874031 , 0.98914116, 0.9858156 ,\n",
       "        0.98877666, 0.99056604, 0.98825832, 0.98654244, 0.986     ]),\n",
       " array([0.99489796, 0.99295154, 0.9874031 , 0.99207921, 0.99083503,\n",
       "        0.98766816, 0.98643006, 0.98249027, 0.97843943, 0.97720515]),\n",
       " array([0.98634294, 0.991641  , 0.9874031 , 0.99060801, 0.98831894,\n",
       "        0.9882221 , 0.98849372, 0.98536585, 0.98247423, 0.98158288]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest?\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 20)                250900    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 256,054\n",
      "Trainable params: 256,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But hooooly that is an enormous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How small can I get the model?\n",
    "This attempt uses less filters and less units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model5.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=16, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "model5.add(Conv2D(filters=8, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(units=10, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model5.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.5342\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1960\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1418\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1163\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0993\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0871\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0774\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0712\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0652\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1506a5c10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model5.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot5 = model5.predict(images_3d_test)\n",
    "\n",
    "predictions_test5 = numpy.argmax(predictions_test_onehot5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=labels_test, y_pred=predictions_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.97883065, 0.98601399, 0.98252427, 0.97553816, 0.98556701,\n",
       "        0.98187995, 0.98526316, 0.98715415, 0.96352584, 0.97029703]),\n",
       " array([0.99081633, 0.9938326 , 0.98062016, 0.98712871, 0.97352342,\n",
       "        0.97197309, 0.97703549, 0.97178988, 0.97638604, 0.97125867]),\n",
       " array([0.98478702, 0.98990785, 0.98157129, 0.98129921, 0.9795082 ,\n",
       "        0.97690141, 0.98113208, 0.97941176, 0.96991331, 0.97077761]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                62730     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 64,160\n",
      "Trainable params: 64,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1% less accuracy for a quarter the size is not bad. Brandon said the best he got was 96% for a 15k parameter model so my goal is to beat that. Also got a 97.9% for 106k params. But first I want to check what led to the huge model size increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 20)                250900    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 256,054\n",
      "Trainable params: 256,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model6.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=32, kernel_size=3, strides=1, padding='same'))\n",
    "model6.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(units=20, activation='relu'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model6.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So removing activation functions doesn't change the number of parameters. That makes sense because it's not an actual variable that you're changing. That means the massive increase in parameters was from reducing the stride size\n",
    "\n",
    "What if I try adding another filter layer but increase stride size in the first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model6.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=16, kernel_size=3, strides=2, padding='same', activation='tanh'))\n",
    "model6.add(Conv2D(filters=8, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "model6.add(Conv2D(filters=4, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(units=10, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model6.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.7744\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2789\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1882\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1488\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1256\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1127\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0998\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0922\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0852\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x115a70970>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model6.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0746\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0723\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0677\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0652\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0624\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0599\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0585\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0554\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0537\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152ef4b20>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot6 = model6.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test6 = numpy.argmax(predictions_test_onehot6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.98681542, 0.98943662, 0.98832685, 0.9764475 , 0.9907312 ,\n",
       "        0.97760358, 0.97803347, 0.98720472, 0.97850563, 0.96954813]),\n",
       " array([0.99285714, 0.99030837, 0.98449612, 0.98514851, 0.9796334 ,\n",
       "        0.97869955, 0.97599165, 0.97568093, 0.98151951, 0.97819623]),\n",
       " array([0.98982706, 0.9898723 , 0.98640777, 0.98077871, 0.98515105,\n",
       "        0.97815126, 0.97701149, 0.981409  , 0.98001025, 0.97385298]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest?\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 14, 14, 4)         292       \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 9,572\n",
      "Trainable params: 9,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wow that is insane!!!\n",
    "A 9k param model at 98.2% is unbelievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for context Brandon I had been talking and competing on how efficient we could get our models, and he had one that was 2.7k params for 96.5% and so I really wanted to beat that. Here are my attempts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model7.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=8, kernel_size=3, strides=2, activation='tanh'))\n",
    "model7.add(Conv2D(filters=8, kernel_size=3, strides=1, activation='tanh'))\n",
    "model7.add(Conv2D(filters=4, kernel_size=2, strides=1, activation='tanh'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(units=10, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model7.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.8582\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.3757\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2598\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2054\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1693\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1480\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1340\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1240\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1143\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x151ef5d60>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model7.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot7 = model7.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test7 = numpy.argmax(predictions_test_onehot7, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.96896897, 0.98318584, 0.94449671, 0.96108949, 0.96396396,\n",
       "        0.97433036, 0.97484277, 0.97308076, 0.96991701, 0.97821577]),\n",
       " array([0.9877551 , 0.97885463, 0.97286822, 0.97821782, 0.98065173,\n",
       "        0.97869955, 0.97077244, 0.94941634, 0.95995893, 0.9345887 ]),\n",
       " array([0.97827185, 0.98101545, 0.95847255, 0.96957802, 0.97223624,\n",
       "        0.97651007, 0.97280335, 0.9611029 , 0.96491228, 0.95590471]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_45 (Conv2D)           (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 11, 11, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 10, 10, 4)         132       \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 10)                4010      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 4,916\n",
      "Trainable params: 4,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm looking at different possible ways to calculate the efficiency of a model, since that was how Brandon and I were comparing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10033.503258829032\n",
      "2474.8679655491565\n"
     ]
    }
   ],
   "source": [
    "print(4916/math.log(100-96.91, 10))\n",
    "print(4916/math.log(96.91, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255317563514.63696"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4916**(100-96.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477074126.80299675"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9572**(100-97.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764906599485.6727"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2720**(100-96.54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I finally did the max pooling that Brandon suggested, and it worked out well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model8.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=16, kernel_size=3, strides=1, activation='tanh'))\n",
    "model8.add(MaxPooling2D(pool_size=2))\n",
    "model8.add(Conv2D(filters=8, kernel_size=3, strides=1, activation='tanh'))\n",
    "model8.add(Conv2D(filters=4, kernel_size=3, strides=1, activation='tanh'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model8.add(Flatten())\n",
    "model8.add(Dense(units=20, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model8.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.5418\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1932\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1440\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1215\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1056\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0941\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0881\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0813\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0756\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x155368ca0>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model8.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0673\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0647\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0622\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0588\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0577\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0578\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0540\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0519\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0496\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152f2a160>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.fit(images_3d_train, labels_onehot_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_onehot8 = model8.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test8 = numpy.argmax(predictions_test_onehot8, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9842"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.98076923, 0.98861646, 0.98158915, 0.98231827, 0.98477157,\n",
       "        0.98759865, 0.98130841, 0.98627451, 0.98345398, 0.98496994]),\n",
       " array([0.98877551, 0.99471366, 0.98158915, 0.99009901, 0.98778004,\n",
       "        0.98206278, 0.98643006, 0.97859922, 0.97638604, 0.97423191]),\n",
       " array([0.9847561 , 0.99165569, 0.98158915, 0.98619329, 0.98627351,\n",
       "        0.98482293, 0.98386257, 0.98242188, 0.97990726, 0.9795715 ]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_66 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 11, 11, 8)         1160      \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 9, 9, 4)           292       \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 324)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 20)                6500      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 8,322\n",
      "Trainable params: 8,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is insanely efficient! I should add that Brandon had sent me a 8.3k param model with 97% accuracy, so I felt proud to say the least. After this model's success I decided to move onto the cats and dogs classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model256k/assets\n",
      "INFO:tensorflow:Assets written to: model64k/assets\n",
      "INFO:tensorflow:Assets written to: model9k/assets\n",
      "INFO:tensorflow:Assets written to: model4k/assets\n",
      "INFO:tensorflow:Assets written to: model8k/assets\n"
     ]
    }
   ],
   "source": [
    "model4.save('model256k')\n",
    "model5.save('model64k')\n",
    "model6.save('model9k')\n",
    "model7.save('model4k')\n",
    "model8.save('model8k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
