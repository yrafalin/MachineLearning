{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model for Newsgroups Data\n",
    "\n",
    "For an explanation of the Naive Bayes model, see [our course notes](https://jennselby.github.io/MachineLearningCourseNotes/#naive-bayes).\n",
    "\n",
    "This notebook uses code from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html.\n",
    "\n",
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Read through the code in the following sections:\n",
    "  * [Newgroups Data](#Newgroups-Data)\n",
    "  * [Model Training](#Model-Training)\n",
    "  * [Prediction](#Prediction)\n",
    "0. Complete at least one of the following exercises:\n",
    "  * [Exercise Option #1 - Standard Difficulty](#Exercise-Option-#1---Standard-Difficulty)\n",
    "  * [Exercise Option #2 - Advanced Difficulty](#Exercise-Option-#2---Advanced-Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # the 20 newgroups set is included in scikit-learn\n",
    "from sklearn.naive_bayes import MultinomialNB # we need this for our Naive Bayes model\n",
    "\n",
    "# These next two are about processing the data. We'll look into this more later in the semester.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newgroups Data\n",
    "\n",
    "Back in the day, [Usenet](https://en.wikipedia.org/wiki/Usenet_newsgroup) was a popular discussion system where people could discuss topics in relevant newsgroups (think Slack channel or subreddit). At some point, someone pulled together messages sent to 20 different newsgroups, to use as [a dataset for doing text processing](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "We are going to pull out messages from just a few different groups to try out a Naive Bayes model.\n",
    "\n",
    "Examine the newsgroups dictionary, to make sure you understand the dataset.\n",
    "\n",
    "**Note**: If you get an error about SSL certificates, you can fix this with the following:\n",
    "1. In Finder, click on Applications in the list on the left panel\n",
    "1. Double click to go into the Python folder (it will be called something like Python 3.7)\n",
    "1. Double click on the Install Certificates command in that folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which newsgroups we want to download\n",
    "newsgroup_names = ['comp.graphics', 'rec.sport.hockey', 'sci.electronics', 'sci.space']\n",
    "\n",
    "# get the newsgroup data (organized much like the iris data)\n",
    "newsgroups = fetch_20newsgroups(categories=newsgroup_names, shuffle=True, random_state=265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "5\n",
      "[\"From: jca2@cec1.wustl.edu (Joseph Charles Achkar)\\nSubject: Blues steal game 1 from Hawks\\nKeywords: Blues, Hull, Shanahan, Joseph, Blackhawks, Belfour\\nNntp-Posting-Host: cec1\\nOrganization: Washington University, St. Louis MO\\nLines: 125\\n\\n\\n  The Blues scored two power-play goals in 17 seconds in the third period\\nand the beat the Chicago Blackhawks 4-3 Sunday afternoon at Chicago Stadium.\\nBrendan Shanahan tied the game 3-3 and Brett Hull scored the game winner 17\\nseconds later. Jeff Brown and Denny Felsner scored the other Blues goals.\\nBrian Noonan had the hat trick for the Hawks, who also had some very good\\ngoaltending from Ed Belfour. Blues goalie Curtis Joseph was solid down the\\nstretch to preserve the Blues lead.\\n\\nThe Hawks came out strong in the first period, outshooting the Blues 6-1 and\\ntaking a 1-0 lead on Noonan's first goal. Right after an interference penalty\\non Rick Zombo had expired, Keith Brown intercepted a clearing attempt at the\\nblue line and passed the puck to Steve Larmer in the right circle. Larmer fired\\na long slap shot, and Noonan deflected the puck between Joseph's pads. After\\nthe goal, the Blues picked up the intensity and went on to outshoot the Hawks\\n10-9 in the first period.\\n\\nJeff Brown tied the game 1-1 at 3:12 of the second. Nelson Emerson broke in on\\nthe left side, got by Craig Muni and pushed the puck across the slot. Belfour\\ncame out to play the pass and shoveled it to the right boards, where Brown\\ncollected it and slapped it in before Belfour could get back to the goal.\\n\\nTwo minutes later on a Hawks power play, Belfour stopped Rich Sutter on a\\nshort-handed break-in. Chris Chelios picked up the puck and passed it to Jeremy\\nRoenick who carried it on right wing and found an open Noonan with a nice pass\\nacross the slot. Noonan fired it past Joseph at 5:30 for the 2-1 lead.\\n\\nNoonan completed his hat trick 3:11 later to increase the Hawks' lead to 3-1.\\nStephane Matteau made a nice pass from the right boards to Noonan who beat\\nStephane Quintal by driving to the net. Joseph had no chance as Noonan\\ndeflected the puck in the net.\\n\\nDenny Felsner reduced the Blues deficit to 3-1 at 12:49 after picking up the\\nrebound of Basil McRae's slap shot from the slot. Janney set up McRae for\\nthe shot, and the puck sailed wide of the net and bounced off the end boards\\nto Felsner. Felsner sticked the rebound into the partially open net. The Blues\\noutshot the Hawks 10-5 in the second period.\\n\\nWith the Blackhawks leading 3-2 at 9:56 of the third, Stephane Matteau picked\\nup a high sticking penalty. Just 53 seconds into the power play, Steve Smith\\nwas called for slashing, giving the Blues a 5-on-3 advantage for 1:07.\\n\\nThe Blues didn't waste time as Brendan Shanahan scored just 23 seconds into\\nthe two-man advantage to tie the game 3-3. Janney found Hull in the slot,\\nand Hull fired a rocket at Belfour. Jeff Brown collected the rebound and\\npassed it to Shanahan in the left circle. Shanahan beat Belfour from a sharp\\nangle.\\n\\nJust 17 seconds later, Hull scored the game winner for the Blues. Nelson\\nEmerson broke in on right wing, carried the puck behind the net along with\\ntwo Hawks defensemen. Emerson made a nice pass to an unchecked Hull in the\\nslot, and Hull beat Belfour to put the Blues up 4-3.\\n\\nThe Hawks had several chances to tie the game in the final minutes, but Joseph\\nmade some brilliant saves to prevent the Hawks from scoring. He stopped Troy\\nMurray point blank from just right of the crease with 2:30 left in the game.\\nThe Blues killed off a late Hawks power play, with Rich Sutter clearing the\\npuck with his hand as it was trickling along the goal line. The Blues held\\non to win the game. The Hawks oushot the Blues 13-7 in the third period,\\ntotaling 27 shots on goal for each team. The Blues special teams were excellent\\nin the game. The Blues killed 6 of 7 Hawks power plays, and scored twice on\\non four power play chances. The Blues ranked among the best special teams in\\nthe league. They rank 2nd in penalty killing and 3rd on the power play.\\n \\nThe game was carried live on ABC, the first time an NHL game other than an\\nAll-Star game has been shown on network television since May 24, 1980, when\\nCBS carried Game 6 of the Stanley Cup finals.\\n\\nThe best-of-seven series continues Wednesday in Chicago and Friday and Sunday\\nin St. Louis.\\n\\nBox score\\n---------\\nBlues 4, Blackhawks 3\\n\\nBLUES       0    2    2   --   4\\nCHICAGO     1    2    0   --   3\\n\\nFIRST PERIOD\\n\\n   CHI -- Noonan 1 (Larmer, K.Brown), 8:17.\\n\\n   Penalties -- Shanahan, StL (holding), 2:28; Zombo, StL (interference), 6:00;\\nMurphy, Chi (high-sticking), 11:30; Grimson, Chi (boarding), 14:39; Zombo, StL\\n(holding), 18:46.\\n\\nSECOND PERIOD\\n\\n   STL -- Brown 1 (Shanahan, Emerson), 3:12.\\n   CHI -- (PPG) Noonan 2 (Roenick, Chelios), 5:40.\\n   CHI -- Noonan 3 (Matteau, Sutter), 8:51.\\n   STL -- Felsner 1 (McRae, Janney), 12:49.\\n\\n   Penalties -- Baron, StL (interference), 4:33; Wilson, StL (tripping), 9:31.\\n\\nTHIRD PERIOD\\n\\n   STL -- (PPG) Shanahan 1 (J.Brown, Hull), 11:12.\\n   STL -- (PPG) Hull 1 (Emerson, J.Brown), 11:29.\\n\\n   Penalties -- Shanahan, StL (roughing), 1:54; Matteau, Chi (high-sticking),\\n9:56; Smith, Chi (slashing), 10:49; Baron, StL (roughing), 14:23.\\n\\nSHOTS ON GOAL\\n\\nBLUES       10    10     7   --   27\\nCHICAGO      9     5    13   --   27\\n\\nPower-play Opportunities -- St. Louis 2 of 4; Chicago 1 of 7.\\n\\nGoaltenders -- St. Louis, Joseph, 1-0-0 (27 shots-24 saves).\\n               Chicago, Belfour, 0-1-0 (27-23).\\n\\nReferee -- Kerry Fraser. Linesmen -- Kevin Collins, Brian Murphy. A -- 16,199.\\n\\n  %*%*%*%**%*%%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*\\n  *   __  ______________         ____________________________________    % \\n  %   \\\\ \\\\_)____________/         A L L E Z   L E S   B L U E S  ! ! !    * \\n  *    \\\\    __________/          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    % \\n  %     \\\\    ________/                                                   *\\n  *      \\\\   _______/                  Joe Ashkar                        % \\n  %       \\\\  \\\\                         Contact for the Blues             *\\n  *        \\\\  \\\\  SAINT LOUIS           jca2@cec1.wustl.edu               % \\n  %        (___)             BLUES                                       * \\n  *%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*%*% \\n\", 'From: jamie@genesis.MCS.COM (James R Groves)\\nSubject: FTP for Targa+\\nOrganization: MCSNet Contributor, Chicago, IL\\nLines: 5\\nDistribution: world\\nNNTP-Posting-Host: localhost.mcs.com\\n\\nI am looking for software to run on my brand new Targa+ 16/32. If anyone knows\\nof any sites which have useful stuff, or if you have any yourself you want to\\ngive, let me know via mail. Thanks a LOT! Yayayay!\\n                                     jamie@ddsw1.mcs.com\\n\\n', \"From: mmb@lamar.ColoState.EDU (Michael Burger)\\nSubject: TV Schedule for Next Week\\nDistribution: na\\nNntp-Posting-Host: lamar.acns.colostate.edu\\nOrganization: Colorado State University, Fort Collins, CO  80523\\nLines: 20\\n\\nUnited States TV Schedule:\\nApril 18   Devils/Islanders at Pittsburgh   1 EST  ABC  (to Eastern time zone)\\nApril 18   St. Louis at Chicago             12 CDT ABC  (to Cent/Mou time zones)\\nApril 18   Los Angeles at Calgary           12 PDT ABC  (to Pacific time zone)\\nApril 20   Devils/Islanders at Pittsburgh   7:30   ESPN\\nApril 22   TBA                              7:30   ESPN\\nApril 24   TBA                              7:30   ESPN\\n\\nIf somebody would send me the CBC/TSN schedule I'll post that as well.\\n\\n\\n*******************************************************************************\\n*  Mike Burger                    *  My Canada includes, Quebec, Ontario,     *\\n*  mmb@lamar.colostate.edu        *  the Maritimes, the Prairies, and Florida *\\n*  A Beginning Computing TA Stud  *  four months a year.                      *\\n*  over 500 students served       *    --Royal Canadian Air Farce             *\\n*******************************************************************************\\n*      University of Michigan - 1990  --  Colorado State University - 199?    *\\n*******************************************************************************\\n\\n\", \"From: gfk39017@uxa.cso.uiuc.edu (George F. Krumins)\\nSubject: Re: Vandalizing the sky.\\nOrganization: University of Illinois at Urbana\\nLines: 59\\n\\nnicho@vnet.IBM.COM (Greg Stewart-Nicholls) writes:\\n\\n>In <C5y4t7.9w3@news.cso.uiuc.edu> George F. Krumins writes:\\n>>It is so typical that the rights of the minority are extinguished by the\\n>>wants of the majority, no matter how ridiculous those wants might be.\\n> Umm, perhaps you could explain what 'rights' we are talking about\\n>here ..\\n> -----------------------------------------------------------------\\n>Greg Nicholls ...         : Vidi\\n>nicho@vnet.ibm.com or     : Vici\\n>nicho@olympus.demon.co.uk : Veni\\n\\nI was suggesting that the minority of professional and amateur astronomers\\nhave the right to a dark, uncluttered night sky.\\n\\nLet me give you an example.  When you watch TV, they have commercials to pay\\nfor the programming.  You accept that as part of watching.  If you don't like\\nit, you can turn it off.  If you want to view the night sky, and there is a\\nfloating billboard out there, you can't turn it off.  It's the same \\nreasoning that limits billboards in scenic areas.\\n\\nPat writes:\\nGeorge.\\n\\n\\tIt's called a democracy.  The majority rules.  sorry.\\nIf ytou don't like it, I suggest you modify the constitution to include\\na constitutional right to Dark Skies.   The theory of government\\nhere is that the majority rules,  except in the nature of fundamental\\ncivil rights.\\n\\nI say: \\n\\tAny reasonably in-depth perusal of American history will show\\n\\tyou that many WASPs have continued the practices of prejudice,\\n\\tdiscrimination, and violence against others of different\\n\\traces, religions, and beliefs, despite the law.\\n\\nPat says:\\nIf you really are annoyed,   get some legislation\\nto create a dark sky zone,  where in all light emissions are protected\\nin the zone.  Kind of like the national radio quiet zone.  Did you\\nknow about that?  near teh Radio telescope  observatory in West virginia,\\nthey have a 90?????? mile EMCON zone.  Theoretically they can prevent\\nyou from running light AC motors, like air conditioners and Vacuums.\\nIn practice, they use it mostly to  control  large radio users.\\n\\nI say:\\nWhat I'm objecting to here is a floating billboard that, presumably,\\nwould move around in the sky.  I, for one, am against legislating\\nat all.  I just wish that people had a bit of common courtesy, and\\nwould consider how their greed for money impacts the more ethereal and\\naesthetic values that make us human.  This includes the need for wild\\nand unspoiled things, including the night sky.\\n\\nGeorge\\n-- \\n|  George Krumins                     /^\\\\        The Serpent and the Rainbow  | \\n|  gfk39017@uxa.cso.uiuc.edu       <^^. .^^>                                  |\\n|  Pufferish Observatory           <_ (o) _>                                  |\\n|                                     \\\\_/                                     | \\n\", 'From: jbh55289@uxa.cso.uiuc.edu (Josh Hopkins)\\nSubject: Re: Lindbergh and the moon (was:Why not give $1G)\\nOrganization: University of Illinois at Urbana\\nLines: 42\\n\\nmancus@sweetpea.jsc.nasa.gov (Keith Mancus) writes:\\n\\n>cook@varmit.mdc.com (Layne Cook) writes:\\n>> All of this talk about a COMMERCIAL space race (i.e. $1G to the first 1-year \\n>> moon base) is intriguing. Similar prizes have influenced aerospace \\n>>development before. The $25k Orteig prize helped Lindbergh sell his Spirit of \\n>> Saint Louis venture to his financial backers.\\n>> But I strongly suspect that his Saint Louis backers had the foresight to \\n>> realize that much more was at stake than $25,000.\\n>> Could it work with the moon? Who are the far-sighted financial backers of \\n>> today?\\n\\n>  The commercial uses of a transportation system between already-settled-\\n>and-civilized areas are obvious.  Spaceflight is NOT in this position.\\n>The correct analogy is not with aviation of the \\'30\\'s, but the long\\n>transocean voyages of the Age of Discovery.\\n\\nLindbergh\\'s flight took place in \\'27, not the thirties.\\n\\n>It didn\\'t require gov\\'t to\\n>fund these as long as something was known about the potential for profit\\n>at the destination.  In practice, some were gov\\'t funded, some were private.\\n\\nCould you give examples of privately funded ones?\\n\\n>But there was no way that any wise investor would spend a large amount\\n>of money on a very risky investment with no idea of the possible payoff.\\n\\nYour logic certainly applies to standard investment strategies.  However, the\\nconcept of a prize for a difficult goal is done for different reasons, I \\nsuspect.  I\\'m not aware that Mr Orteig received any significant economic \\nbenefit from Lindbergh\\'s flight.  Modern analogies, such as the prize for a\\nhuman powered helicopter face similar arguments.  There is little economic\\nbenefit in such a thing.  The advantage comes in the new approaches developed\\nand the fact that a prize will frequently generate far more work than the \\nequivalent amount of direct investment would.  A person who puts up $ X billion\\nfor a moon base is much more likely to do it because they want to see it done\\nthan because they expect to make money off the deal.\\n-- \\nJosh Hopkins                                          jbh55289@uxa.cso.uiuc.edu\\n\\t\\t    \"Find a way or make one.\"\\n\\t             -attributed to Hannibal\\n', 'From: Amruth Laxman <al26+@andrew.cmu.edu>\\nSubject: Surviving Large Accelerations?\\nOrganization: Junior, Electrical and Computer Engineering, Carnegie Mellon, Pittsburgh, PA\\nLines: 16\\nNNTP-Posting-Host: po5.andrew.cmu.edu\\n\\nHi,\\n    I was reading through \"The Spaceflight Handbook\" and somewhere in\\nthere the author discusses solar sails and the forces acting on them\\nwhen and if they try to gain an initial acceleration by passing close to\\nthe sun in a hyperbolic orbit. The magnitude of such accelerations he\\nestimated to be on the order of 700g. He also says that this is may not\\nbe a big problem for manned craft because humans (and this was published\\nin 1986) have already withstood accelerations of 45g. All this is very\\nlong-winded but here\\'s my question finally - Are 45g accelerations in\\nfact humanly tolerable? - with the aid of any mechanical devices of\\ncourse. If these are possible, what is used to absorb the acceleration?\\nCan this be extended to larger accelerations?\\n\\nThanks is advance...\\n-Amruth Laxman\\n\\n', 'From: kwp@wag.caltech.edu (Kevin W. Plaxco)\\nSubject: Re: Boom!  Whoosh......\\nOrganization: California Institute of Technology, Pasadena, CA\\nLines: 22\\nNNTP-Posting-Host: sgi1.wag.caltech.edu\\n\\nIn article <37147@scicom.AlphaCDC.COM> wats@scicom.AlphaCDC.COM (Bruce Watson) writes:\\n>+\\n>Pageos and two Echo balloons were inflated with a substance\\n>which expanded in vacuum. \\n\\nCalled \"gas\".\\n\\n>Once inflated the substance was no longer\\n>needed since there is nothing to cause the balloon to collapse.\\n>This inflatable structure could suffer multiple holes with no \\n>disastrous deflation.\\n\\nThe balloons were in sufficiently low orbit that they experienced\\nsome air resistance.  When they were finally punctured, this \\npreasure (and the internal preasure that was needed to maintain\\na spherical shape against this resistance) caused them to\\ncatastrophically deflated.  The large silvered shards\\nthat remained were easily visible for some time before\\nreentry, though no longer useful as a passive transponder.\\n\\nThe billboard should pop like a dime store balloon.\\n\\n', 'From: wbdst+@pitt.edu (William B Dwinnell)\\nSubject: VESA as a graphics standard\\nOrganization: University of Pittsburgh\\nLines: 7\\n\\n\\nIn the U\\x08IBM PC world, how much of a \"standard\" has VESA become for\\nSVGA graphics?  I know there are lots of graphics-board companies out \\nthere, as well as several graphics chips manufacturers- are they adhering to\\nthe VESA standard, and what effect is/will the VESA Local Bus have on all\\nof this?\\nAnyone?\\n', \"From: n4hy@harder.ccr-p.ida.org (Bob McGwier)\\nSubject: Re: NAVSTAR positions\\nOrganization: IDA Center for Communications Research\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: harder.ccr-p.ida.org\\nIn-reply-to: Thomas.Enblom@eos.ericsson.se's message of 19 Apr 93 06:34:55 GMT\\n\\n\\nYou have missed something.  There is a big difference between being in\\nthe SAME PLANE and in exactly the same state (positions and velocities\\nequal).  IN addition to this, there has always been redundancies proposed.\\n\\nBob\\n--\\n------------------------------------------------------------------------------\\nRobert W. McGwier                  | n4hy@ccr-p.ida.org\\nCenter for Communications Research | Interests: amateur radio, astronomy,golf\\nPrinceton, N.J. 08520              | Asst Scoutmaster Troop 5700, Hightstown\\n\", \"Organization: University of Maine System\\nFrom: The Always Fanatical: Patrick Ellis <IO11330@MAINE.MAINE.EDU>\\nSubject: Keenan signs, Plus WALSH????????\\n <1993Apr16.235100.18268@pasteur.Berkeley.EDU>\\n <93108.121926IO11330@MAINE.MAINE.EDU>\\nLines: 25\\n\\n\\nWell I just read in the Boston Globe that while not confirming\\n(or denying) anything, Walsh may end up with the Rangers organizations\\nas an (assistant Coach?).  Keenan has talked with Walsh in the past\\n(he came up to see Kariya as he will be coaching him in the worlds,\\nfunny I guess he got to watch the Ferraro brothers as well.....) I'm\\nnot sure if walsh will go, but if Keenan is getting 700,000 and walsh\\neven gets 100,000 that's a 30% pay raise for walsh (not to mention\\na nice career move....) Anyone from New York Hear anything about\\nthis????????\\n\\n               Pat Ellis\\n\\n\\n\\n\\nP.S.  GO BRUINS    GO UMAINE BLACK BEARS    42-1-2       NUMBER 1......\\n\\n                   HOCKEY EAST REGULARS SEASON CHAMPIONS.....\\n                   HOCKEY EAST TOURNAMENT CHAMPIONS>......\\n                   PAUL KARIYA, HOBEY BAKER AWARD WINNER.......\\n         NCAA DIV. 1 HOCKEY TOURNAMENT CHAMPIONS!!!!!!!!!!!!!!!!!!!\\n\\n\\n                    M-A-I-N-E      GGGGOOOOOOO    BBBLLLUUEEEE!\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.keys())\n",
    "print(len(newsgroups))\n",
    "print(newsgroups['data'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/yoarafa/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/53622'\n",
      " '/Users/yoarafa/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38737'\n",
      " '/Users/yoarafa/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/53704'\n",
      " ...\n",
      " '/Users/yoarafa/scikit_learn_data/20news_home/20news-bydate-train/sci.electronics/53811'\n",
      " '/Users/yoarafa/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/53726'\n",
      " '/Users/yoarafa/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38497']\n",
      "['comp.graphics', 'rec.sport.hockey', 'sci.electronics', 'sci.space']\n",
      "[1 0 1 3 3 3 3 0 3 1 0 3 3 1 0 3 3 3 2 0]\n",
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups['filenames'])\n",
    "print(newsgroups['target_names'])\n",
    "print(newsgroups['target'][:20])\n",
    "print(newsgroups['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part does some processing of the data, because the scikit-learn Naive Bayes module is expecting numerical data rather than text data. We will talk more about what this code is doing later in the semester. For now, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text into numbers that represent each word (bag of words method)\n",
    "word_vector = CountVectorizer()\n",
    "word_vector_counts = word_vector.fit_transform(newsgroups.data)\n",
    "\n",
    "# Account for the length of the documents:\n",
    "#   get the frequency with which the word occurs instead of the raw number of times\n",
    "term_freq_transformer = TfidfTransformer()\n",
    "term_freq = term_freq_transformer.fit_transform(word_vector_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we fit the Naive Bayes model to the subset of the 20 newsgroups data that we've pulled out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes model\n",
    "model = MultinomialNB().fit(term_freq, newsgroups.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Let's see how the model does on some (very short) documents that we made up to fit into the specific categories our model is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "\tThat GPU has amazing performance with a lot of shaders => comp.graphics\n",
      "\tThe player had a wicked slap shot => rec.sport.hockey\n",
      "\tI spent all day yesterday soldering banks of capacitors => sci.space\n",
      "\tToday I have to solder a bank of capacitors => sci.electronics\n",
      "\tNASA has rovers on Mars => sci.space\n",
      "Probabilities:\n",
      "comp.graphics    rec.sport.hockey sci.electronics  sci.space        \n",
      "0.29466149       0.22895149       0.24926344       0.22712357       \n",
      "0.12948055       0.51155698       0.18248712       0.17647535       \n",
      "0.18604814       0.24117771       0.27540452       0.29736963       \n",
      "0.21285086       0.21081302       0.3486507        0.22768541       \n",
      "0.079185633      0.066225915      0.10236622       0.75222223       \n"
     ]
    }
   ],
   "source": [
    "# Predict some new fake documents\n",
    "fake_docs = [\n",
    "    'That GPU has amazing performance with a lot of shaders',\n",
    "    'The player had a wicked slap shot',\n",
    "    'I spent all day yesterday soldering banks of capacitors',\n",
    "    'Today I have to solder a bank of capacitors',\n",
    "    'NASA has rovers on Mars']\n",
    "fake_counts = word_vector.transform(fake_docs)\n",
    "fake_term_freq = term_freq_transformer.transform(fake_counts)\n",
    "\n",
    "predicted = model.predict(fake_term_freq)\n",
    "print('Predictions:')\n",
    "for doc, group in zip(fake_docs, predicted):\n",
    "    print('\\t{0} => {1}'.format(doc, newsgroups.target_names[group]))\n",
    "\n",
    "probabilities = model.predict_proba(fake_term_freq)\n",
    "print('Probabilities:')\n",
    "print(''.join(['{:17}'.format(name) for name in newsgroups.target_names]))\n",
    "for probs in probabilities:\n",
    "    print(''.join(['{:<17.8}'.format(prob) for prob in probs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #1 - Standard Difficulty\n",
    "\n",
    "Modify the fake documents and add some new documents of your own. \n",
    "\n",
    "What words in your documents have particularly large effects on the model probabilities? Note that we're not looking for documents that consist of a single word, but for words that, when included or excluded from a document, tend to change the model's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "\tThat has amazing performance with a lot of shaders => comp.graphics\n",
      "\tThe player had a wicked slap shot NASA => rec.sport.hockey\n",
      "\tI spent all day yesterday soldering banks of capacitors electronics => sci.electronics\n",
      "\tToday I have to solder a bank of capacitors on Mars => sci.space\n",
      "\tI love using ray casting for the ice rink => rec.sport.hockey\n",
      "\tray casting => comp.graphics\n",
      "\tray casting mars => sci.space\n",
      "\tray casting capacitors => sci.electronics\n",
      "\tGPU => rec.sport.hockey\n",
      "\tGPU capacitors => sci.electronics\n",
      "\tGPU mars => sci.space\n",
      "\tGPU puck => rec.sport.hockey\n",
      "\tI spent all day yesterday banks of capacitors => sci.space\n",
      "\tI spent all day yesterday soldering of capacitors => sci.space\n",
      "\tI all day yesterday soldering banks of capacitors => sci.electronics\n",
      "\tI spent all yesterday soldering banks of capacitors => sci.electronics\n",
      "\tI spent all day yesterday soldering banks of capacitor => sci.space\n",
      "\tI spent all day soldering banks of capacitors => sci.electronics\n",
      "\tI spent all day yesterday => sci.space\n",
      "Probabilities:\n",
      "comp.graphics    rec.sport.hockey sci.electronics  sci.space        \n",
      "0.29466149       0.22895149       0.24926344       0.22712357       \n",
      "0.12800355       0.40517136       0.19360437       0.27322072       \n",
      "0.15992056       0.19604879       0.37859843       0.26543223       \n",
      "0.17768789       0.17582281       0.29048531       0.35600399       \n",
      "0.2907754        0.35098116       0.16603719       0.19220625       \n",
      "0.38941409       0.18806058       0.14355689       0.27896844       \n",
      "0.20812918       0.12259259       0.11246085       0.55681738       \n",
      "0.27580354       0.17880922       0.32207209       0.22331516       \n",
      "0.24662162       0.25337838       0.2495777        0.2504223        \n",
      "0.18226253       0.1828025        0.45496284       0.17997212       \n",
      "0.12198303       0.11727265       0.13222714       0.62851718       \n",
      "0.12166571       0.63641236       0.12178513       0.12013679       \n",
      "0.18223337       0.24761404       0.25423702       0.31591557       \n",
      "0.18001519       0.24460003       0.27458816       0.30079663       \n",
      "0.18310787       0.25445814       0.31268821       0.24974578       \n",
      "0.20447264       0.23571803       0.2859944        0.27381492       \n",
      "0.18464477       0.23993255       0.2790246        0.29639808       \n",
      "0.18678166       0.21152007       0.3082708        0.29342747       \n",
      "0.1796208        0.28977182       0.14531709       0.38529029       \n"
     ]
    }
   ],
   "source": [
    "# Predict some new fake documents\n",
    "new_fake_docs = [\n",
    "    'That has amazing performance with a lot of shaders',\n",
    "    'The player had a wicked slap shot NASA',\n",
    "    'I spent all day yesterday soldering banks of capacitors electronics',\n",
    "    'Today I have to solder a bank of capacitors on Mars',\n",
    "    'I love using ray casting for the ice rink',\n",
    "    'ray casting',\n",
    "    'ray casting mars',\n",
    "    'ray casting capacitors',\n",
    "    'GPU',\n",
    "    'GPU capacitors',\n",
    "    'GPU mars',\n",
    "    'GPU puck',\n",
    "    'I spent all day yesterday banks of capacitors',\n",
    "    'I spent all day yesterday soldering of capacitors',\n",
    "    'I all day yesterday soldering banks of capacitors',\n",
    "    'I spent all yesterday soldering banks of capacitors',\n",
    "    'I spent all day yesterday soldering banks of capacitor',\n",
    "    'I spent all day soldering banks of capacitors',\n",
    "    'I spent all day yesterday'\n",
    "]\n",
    "new_fake_counts = word_vector.transform(new_fake_docs)\n",
    "new_fake_term_freq = term_freq_transformer.transform(new_fake_counts)\n",
    "\n",
    "predicted = model.predict(new_fake_term_freq)\n",
    "print('Predictions:')\n",
    "for doc, group in zip(new_fake_docs, predicted):\n",
    "    print('\\t{0} => {1}'.format(doc, newsgroups.target_names[group]))\n",
    "\n",
    "probabilities = model.predict_proba(new_fake_term_freq)\n",
    "print('Probabilities:')\n",
    "print(''.join(['{:17}'.format(name) for name in newsgroups.target_names]))\n",
    "for probs in probabilities:\n",
    "    print(''.join(['{:<17.8}'.format(prob) for prob in probs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very difficult to determine which words have a stronger effect without looking at the models itself, but I can extrapolate some things from my experiments above. For one, GPU and ray casting very weakly affect the model towards graphics (CORRECTION: it seems like GPU wasn't even in the dataset). Mars and puck had strong leanings towards the fields they come from. Capacitors was interesting because on one hand it affected many statements strongly towards electronics, but at the same time in a sentence with spent, day, and yesterday, it was not strong enough to change the output away from space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #2 - Advanced Difficulty\n",
    "\n",
    "Write some code to count up how often the words you found in the exercise above appear in each category in the training dataset. Does this match up with your intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics_counts = word_vector_counts.toarray()[newsgroups['target'].T==0].sum(axis=0)\n",
    "hockey_counts = word_vector_counts.toarray()[newsgroups['target'].T==1].sum(axis=0)\n",
    "electronics_counts = word_vector_counts.toarray()[newsgroups['target'].T==2].sum(axis=0)\n",
    "space_counts = word_vector_counts.toarray()[newsgroups['target'].T==3].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 17,  0, ...,  1,  0,  0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphics_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000',\n",
       " '000000',\n",
       " '000005102000',\n",
       " '000021',\n",
       " '000062david42',\n",
       " '000100255pixel',\n",
       " '000256',\n",
       " '00041032',\n",
       " '0004136',\n",
       " '0004246',\n",
       " '0004422',\n",
       " '00044513',\n",
       " '0004847546',\n",
       " '0005',\n",
       " '0007',\n",
       " '00090711',\n",
       " '000usd',\n",
       " '001',\n",
       " '0010580b',\n",
       " '0012',\n",
       " '001200201pixel',\n",
       " '001323',\n",
       " '001428',\n",
       " '001555',\n",
       " '001718',\n",
       " '001757',\n",
       " '0018',\n",
       " '00196',\n",
       " '002',\n",
       " '0020',\n",
       " '0022',\n",
       " '0028',\n",
       " '0029',\n",
       " '00309',\n",
       " '003221',\n",
       " '0033',\n",
       " '0034',\n",
       " '003719',\n",
       " '0038',\n",
       " '003800',\n",
       " '0039',\n",
       " '004418',\n",
       " '0049',\n",
       " '005',\n",
       " '005150',\n",
       " '005512',\n",
       " '0059',\n",
       " '006',\n",
       " '0065',\n",
       " '007',\n",
       " '0078',\n",
       " '008',\n",
       " '0086',\n",
       " '0094',\n",
       " '00969fba',\n",
       " '0098',\n",
       " '00index',\n",
       " '00pm',\n",
       " '00r',\n",
       " '01',\n",
       " '0100',\n",
       " '01002',\n",
       " '010305',\n",
       " '010326',\n",
       " '010821',\n",
       " '011',\n",
       " '011605',\n",
       " '011720',\n",
       " '013',\n",
       " '013653rap115',\n",
       " '013846',\n",
       " '013939',\n",
       " '014',\n",
       " '014237',\n",
       " '014305',\n",
       " '014506',\n",
       " '01463',\n",
       " '0150',\n",
       " '015225',\n",
       " '015415',\n",
       " '015936',\n",
       " '016',\n",
       " '01609',\n",
       " '0164',\n",
       " '01752',\n",
       " '01760',\n",
       " '01775',\n",
       " '01776',\n",
       " '0179',\n",
       " '01821',\n",
       " '01826',\n",
       " '0184',\n",
       " '01852',\n",
       " '01854',\n",
       " '01890',\n",
       " '018b',\n",
       " '0192',\n",
       " '0199',\n",
       " '01a',\n",
       " '01wb',\n",
       " '02',\n",
       " '020',\n",
       " '0200',\n",
       " '020021',\n",
       " '020259',\n",
       " '020359',\n",
       " '020555',\n",
       " '020637',\n",
       " '020751',\n",
       " '021',\n",
       " '02115',\n",
       " '02138',\n",
       " '02139',\n",
       " '02142',\n",
       " '02154',\n",
       " '021708',\n",
       " '02178',\n",
       " '022',\n",
       " '022113',\n",
       " '0223',\n",
       " '0226',\n",
       " '022922',\n",
       " '0233',\n",
       " '0235',\n",
       " '0237',\n",
       " '023843',\n",
       " '023b',\n",
       " '024036',\n",
       " '024423',\n",
       " '0245',\n",
       " '025240',\n",
       " '0273',\n",
       " '02790',\n",
       " '0280',\n",
       " '0293',\n",
       " '03',\n",
       " '030',\n",
       " '0300',\n",
       " '0303',\n",
       " '03051',\n",
       " '031823',\n",
       " '031840',\n",
       " '031905saundrsg',\n",
       " '032017',\n",
       " '032350',\n",
       " '0324',\n",
       " '033',\n",
       " '0330',\n",
       " '0335',\n",
       " '033843',\n",
       " '034',\n",
       " '034101',\n",
       " '0349',\n",
       " '0358',\n",
       " '036',\n",
       " '0362',\n",
       " '037',\n",
       " '038',\n",
       " '0391',\n",
       " '04',\n",
       " '040',\n",
       " '0400',\n",
       " '040286',\n",
       " '040493161915',\n",
       " '040819',\n",
       " '0410',\n",
       " '04110',\n",
       " '041300',\n",
       " '041493003715',\n",
       " '041505',\n",
       " '042',\n",
       " '0423',\n",
       " '042918',\n",
       " '043426',\n",
       " '0435',\n",
       " '043740',\n",
       " '044045',\n",
       " '044323',\n",
       " '044636',\n",
       " '044946',\n",
       " '045',\n",
       " '045046',\n",
       " '0453',\n",
       " '0458',\n",
       " '047',\n",
       " '047m',\n",
       " '0483',\n",
       " '04g',\n",
       " '05',\n",
       " '050',\n",
       " '0500',\n",
       " '051',\n",
       " '0511',\n",
       " '051201',\n",
       " '051309',\n",
       " '051746',\n",
       " '051942',\n",
       " '052120rap115',\n",
       " '0524',\n",
       " '052907',\n",
       " '053',\n",
       " '0531',\n",
       " '053250',\n",
       " '053333',\n",
       " '053736',\n",
       " '053748rap115',\n",
       " '05402',\n",
       " '0541',\n",
       " '05446',\n",
       " '0545',\n",
       " '054589e',\n",
       " '054600',\n",
       " '055',\n",
       " '0570',\n",
       " '058',\n",
       " '059',\n",
       " '06',\n",
       " '060',\n",
       " '0600',\n",
       " '060010',\n",
       " '060043',\n",
       " '060493161931',\n",
       " '060493164354',\n",
       " '0605',\n",
       " '06066',\n",
       " '0608',\n",
       " '061',\n",
       " '06108',\n",
       " '06111',\n",
       " '061329',\n",
       " '06179397',\n",
       " '062622',\n",
       " '0628',\n",
       " '062802',\n",
       " '063253',\n",
       " '0636',\n",
       " '064720',\n",
       " '06487',\n",
       " '0649',\n",
       " '065',\n",
       " '0666',\n",
       " '067',\n",
       " '0674',\n",
       " '0678',\n",
       " '068',\n",
       " '0684',\n",
       " '0695',\n",
       " '06paul',\n",
       " '07',\n",
       " '070',\n",
       " '0700',\n",
       " '0702',\n",
       " '0704',\n",
       " '07059',\n",
       " '07090',\n",
       " '071',\n",
       " '0715',\n",
       " '071549',\n",
       " '072',\n",
       " '0721',\n",
       " '072429',\n",
       " '072523',\n",
       " '072706',\n",
       " '0729',\n",
       " '073134',\n",
       " '073540',\n",
       " '073716',\n",
       " '0739',\n",
       " '074',\n",
       " '074054',\n",
       " '07410',\n",
       " '07653',\n",
       " '077',\n",
       " '0789',\n",
       " '08',\n",
       " '080719',\n",
       " '08080',\n",
       " '0813',\n",
       " '0815',\n",
       " '0820',\n",
       " '082502acps6992',\n",
       " '0833',\n",
       " '083324',\n",
       " '083731',\n",
       " '08502',\n",
       " '08520',\n",
       " '085337',\n",
       " '08540',\n",
       " '085435',\n",
       " '08544',\n",
       " '0856e16',\n",
       " '085848',\n",
       " '0865',\n",
       " '08786',\n",
       " '0883',\n",
       " '089',\n",
       " '0891',\n",
       " '08934',\n",
       " '09',\n",
       " '090',\n",
       " '0900',\n",
       " '090030',\n",
       " '0901',\n",
       " '0903',\n",
       " '090626',\n",
       " '091051',\n",
       " '091503rap115',\n",
       " '091859',\n",
       " '092051',\n",
       " '092246dlmqc',\n",
       " '0926',\n",
       " '092913',\n",
       " '0930',\n",
       " '093828',\n",
       " '094',\n",
       " '0941',\n",
       " '0943',\n",
       " '094320',\n",
       " '094815mece7187',\n",
       " '095653',\n",
       " '0962',\n",
       " '097',\n",
       " '0970',\n",
       " '0971',\n",
       " '0988',\n",
       " '099',\n",
       " '0_',\n",
       " '0______________________________________________________________________0',\n",
       " '0a',\n",
       " '0b',\n",
       " '0b16',\n",
       " '0e9',\n",
       " '0fovj7i00wb4miumht',\n",
       " '0hb',\n",
       " '0km',\n",
       " '0ma',\n",
       " '0p6a3b1w165w',\n",
       " '0v',\n",
       " '0w',\n",
       " '0w0',\n",
       " '0x',\n",
       " '0x00',\n",
       " '0x100',\n",
       " '0x1f',\n",
       " '0x3d4',\n",
       " '0xc010',\n",
       " '0xc018',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '100014',\n",
       " '10007',\n",
       " '10009',\n",
       " '1000hz',\n",
       " '1000r',\n",
       " '1001',\n",
       " '10012',\n",
       " '10019',\n",
       " '10020',\n",
       " '10022',\n",
       " '10023',\n",
       " '1003',\n",
       " '10036',\n",
       " '10038',\n",
       " '1004',\n",
       " '100452',\n",
       " '1005',\n",
       " '1006',\n",
       " '100640',\n",
       " '1007',\n",
       " '1008',\n",
       " '100c',\n",
       " '100db',\n",
       " '100hz',\n",
       " '100k',\n",
       " '100km',\n",
       " '100lez',\n",
       " '100m',\n",
       " '100ma',\n",
       " '100mb',\n",
       " '100megs',\n",
       " '100mfd',\n",
       " '100mhz',\n",
       " '100nm',\n",
       " '100th',\n",
       " '100uf',\n",
       " '101',\n",
       " '1010',\n",
       " '101010',\n",
       " '101044',\n",
       " '1011',\n",
       " '10118',\n",
       " '1012',\n",
       " '1013',\n",
       " '101323',\n",
       " '1014',\n",
       " '1015',\n",
       " '10158',\n",
       " '1016',\n",
       " '1017',\n",
       " '101747',\n",
       " '10176',\n",
       " '10179',\n",
       " '1018',\n",
       " '10182',\n",
       " '1019',\n",
       " '101h',\n",
       " '102',\n",
       " '1020',\n",
       " '102007',\n",
       " '10206',\n",
       " '1021',\n",
       " '1022',\n",
       " '1023',\n",
       " '1024',\n",
       " '10248b',\n",
       " '1024x1024',\n",
       " '1024x512',\n",
       " '1024x728',\n",
       " '1024x768',\n",
       " '1024x768x24',\n",
       " '1024x768x65000',\n",
       " '1024x786x24',\n",
       " '1025',\n",
       " '10250',\n",
       " '1026',\n",
       " '10263',\n",
       " '1027',\n",
       " '102756',\n",
       " '1028',\n",
       " '10281',\n",
       " '1029',\n",
       " '102a',\n",
       " '103',\n",
       " '1030',\n",
       " '103038',\n",
       " '10308',\n",
       " '1031',\n",
       " '1032',\n",
       " '1033',\n",
       " '1034',\n",
       " '1035',\n",
       " '1036',\n",
       " '1037',\n",
       " '1038',\n",
       " '1039',\n",
       " '103953',\n",
       " '104',\n",
       " '1040',\n",
       " '1041',\n",
       " '104107',\n",
       " '1042',\n",
       " '1043',\n",
       " '1044',\n",
       " '1045',\n",
       " '1046',\n",
       " '10460',\n",
       " '1047',\n",
       " '1048',\n",
       " '1049',\n",
       " '105',\n",
       " '1050',\n",
       " '10505',\n",
       " '1051',\n",
       " '10510',\n",
       " '10511',\n",
       " '1052',\n",
       " '1053',\n",
       " '105366',\n",
       " '1054',\n",
       " '1055',\n",
       " '10553',\n",
       " '10555',\n",
       " '1056',\n",
       " '1057',\n",
       " '105738',\n",
       " '1058',\n",
       " '1059',\n",
       " '105m',\n",
       " '106',\n",
       " '1060',\n",
       " '10601',\n",
       " '1061',\n",
       " '1062',\n",
       " '1063',\n",
       " '1064',\n",
       " '1065',\n",
       " '1066',\n",
       " '10665',\n",
       " '1067',\n",
       " '10671',\n",
       " '1068',\n",
       " '10680',\n",
       " '106816',\n",
       " '1069',\n",
       " '106ps',\n",
       " '107',\n",
       " '1070',\n",
       " '1071',\n",
       " '1072',\n",
       " '1073',\n",
       " '1074',\n",
       " '1075',\n",
       " '1076',\n",
       " '1077',\n",
       " '1078',\n",
       " '1079',\n",
       " '108',\n",
       " '1080',\n",
       " '1081',\n",
       " '1082',\n",
       " '1083',\n",
       " '1084',\n",
       " '1085',\n",
       " '1086',\n",
       " '1087',\n",
       " '1088',\n",
       " '10886',\n",
       " '1089',\n",
       " '108mhz',\n",
       " '109',\n",
       " '1090',\n",
       " '1091',\n",
       " '109125',\n",
       " '109147',\n",
       " '1092',\n",
       " '1094',\n",
       " '1095',\n",
       " '1096',\n",
       " '1098',\n",
       " '1099',\n",
       " '10bps',\n",
       " '10c',\n",
       " '10cm',\n",
       " '10dbv',\n",
       " '10fps',\n",
       " '10h',\n",
       " '10k',\n",
       " '10khz',\n",
       " '10km',\n",
       " '10kw',\n",
       " '10m',\n",
       " '10mhz',\n",
       " '10mil',\n",
       " '10min',\n",
       " '10s',\n",
       " '10th',\n",
       " '10v',\n",
       " '10vdc',\n",
       " '10w',\n",
       " '10x',\n",
       " '10xlog',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '110048',\n",
       " '1101',\n",
       " '1102',\n",
       " '1103',\n",
       " '1104',\n",
       " '11048',\n",
       " '1105',\n",
       " '1106',\n",
       " '1107',\n",
       " '11074',\n",
       " '1108',\n",
       " '1109',\n",
       " '110hz',\n",
       " '110m',\n",
       " '110mbytes',\n",
       " '110v',\n",
       " '110vac',\n",
       " '111',\n",
       " '1110',\n",
       " '1111',\n",
       " '1112',\n",
       " '1113',\n",
       " '1114',\n",
       " '1115',\n",
       " '11150',\n",
       " '1116',\n",
       " '1117',\n",
       " '1118',\n",
       " '1119',\n",
       " '111s',\n",
       " '112',\n",
       " '1120',\n",
       " '1121',\n",
       " '1122',\n",
       " '11226',\n",
       " '11230',\n",
       " '1125',\n",
       " '1126',\n",
       " '1127',\n",
       " '1128',\n",
       " '113',\n",
       " '11303',\n",
       " '11311',\n",
       " '1132',\n",
       " '113223',\n",
       " '1133',\n",
       " '1134',\n",
       " '1135',\n",
       " '1136',\n",
       " '1137',\n",
       " '1138',\n",
       " '113953',\n",
       " '114',\n",
       " '1140',\n",
       " '114041',\n",
       " '1141',\n",
       " '11414',\n",
       " '114158',\n",
       " '1142',\n",
       " '1144',\n",
       " '114428',\n",
       " '1145',\n",
       " '1145040',\n",
       " '1146',\n",
       " '1147',\n",
       " '1149',\n",
       " '115',\n",
       " '1150',\n",
       " '115072',\n",
       " '1151',\n",
       " '1152',\n",
       " '11530',\n",
       " '115330',\n",
       " '115331',\n",
       " '115397',\n",
       " '1154',\n",
       " '11544',\n",
       " '11548',\n",
       " '1155',\n",
       " '1156',\n",
       " '1157',\n",
       " '1158',\n",
       " '115868',\n",
       " '115873',\n",
       " '1159',\n",
       " '115a',\n",
       " '115m',\n",
       " '116',\n",
       " '1160',\n",
       " '1161',\n",
       " '11611',\n",
       " '1162',\n",
       " '1163',\n",
       " '11632',\n",
       " '1164',\n",
       " '1165',\n",
       " '11670',\n",
       " '11683',\n",
       " '1169',\n",
       " '117',\n",
       " '1170',\n",
       " '11716',\n",
       " '1172',\n",
       " '1173',\n",
       " '1174',\n",
       " '1175',\n",
       " '1176',\n",
       " '1177',\n",
       " '1178',\n",
       " '1179',\n",
       " '117v',\n",
       " '118',\n",
       " '1180',\n",
       " '11800',\n",
       " '1181',\n",
       " '1182',\n",
       " '1183',\n",
       " '1184',\n",
       " '118467',\n",
       " '1185',\n",
       " '118520',\n",
       " '1186',\n",
       " '11861',\n",
       " '1187',\n",
       " '1188',\n",
       " '11888',\n",
       " '119',\n",
       " '1190',\n",
       " '1192',\n",
       " '1193',\n",
       " '1194',\n",
       " '1196',\n",
       " '11964',\n",
       " '1198',\n",
       " '1199',\n",
       " '119th',\n",
       " '11dec89',\n",
       " '11th',\n",
       " '11x17',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000',\n",
       " '1200mi',\n",
       " '1200x900',\n",
       " '1201',\n",
       " '1202',\n",
       " '120311',\n",
       " '120365',\n",
       " '1204',\n",
       " '120466',\n",
       " '1205',\n",
       " '1206',\n",
       " '120664',\n",
       " '120666',\n",
       " '1207',\n",
       " '1208',\n",
       " '1209',\n",
       " '12091',\n",
       " '12092',\n",
       " '120921',\n",
       " '120rtn_____________________',\n",
       " '120v',\n",
       " '120vac',\n",
       " '121',\n",
       " '1210',\n",
       " '1211',\n",
       " '1212',\n",
       " '121236',\n",
       " '12134',\n",
       " '1214',\n",
       " '121411',\n",
       " '12144',\n",
       " '1215',\n",
       " '121843',\n",
       " '1219',\n",
       " '121925',\n",
       " '121926io11330',\n",
       " '122',\n",
       " '1220',\n",
       " '122037',\n",
       " '1220am',\n",
       " '1221',\n",
       " '1222',\n",
       " '1223',\n",
       " '1225',\n",
       " '1227',\n",
       " '12288',\n",
       " '1229',\n",
       " '122nd',\n",
       " '123',\n",
       " '1230',\n",
       " '1230am',\n",
       " '1231',\n",
       " '12314',\n",
       " '1232',\n",
       " '1234',\n",
       " '123433',\n",
       " '1235',\n",
       " '1236',\n",
       " '1237',\n",
       " '1238',\n",
       " '123832',\n",
       " '1239',\n",
       " '124',\n",
       " '1240',\n",
       " '124012',\n",
       " '1240am',\n",
       " '1241',\n",
       " '1242',\n",
       " '12424',\n",
       " '12426',\n",
       " '1243',\n",
       " '1244',\n",
       " '124456',\n",
       " '1245',\n",
       " '1246',\n",
       " '1247',\n",
       " '124722',\n",
       " '124724',\n",
       " '124759',\n",
       " '1249',\n",
       " '125',\n",
       " '12500',\n",
       " '1251',\n",
       " '125147',\n",
       " '1253',\n",
       " '1254',\n",
       " '1255',\n",
       " '125512',\n",
       " '125530',\n",
       " '1256',\n",
       " '125608',\n",
       " '1257',\n",
       " '125750',\n",
       " '1259',\n",
       " '125951pca103',\n",
       " '126',\n",
       " '12602',\n",
       " '12607',\n",
       " '1261',\n",
       " '12613',\n",
       " '1262',\n",
       " '1263',\n",
       " '12649',\n",
       " '1268',\n",
       " '127',\n",
       " '1270',\n",
       " '12704w',\n",
       " '1270am',\n",
       " '1272',\n",
       " '1275',\n",
       " '12770',\n",
       " '1279',\n",
       " '128',\n",
       " '1280',\n",
       " '1280am',\n",
       " '1281',\n",
       " '1282',\n",
       " '12821',\n",
       " '1283',\n",
       " '1285',\n",
       " '1286',\n",
       " '1287',\n",
       " '1288',\n",
       " '12889',\n",
       " '1289',\n",
       " '128k',\n",
       " '128m',\n",
       " '129',\n",
       " '1290',\n",
       " '1291',\n",
       " '1292',\n",
       " '12934',\n",
       " '1294',\n",
       " '1296',\n",
       " '1297',\n",
       " '1298',\n",
       " '1299',\n",
       " '12a',\n",
       " '12ga',\n",
       " '12km',\n",
       " '12m',\n",
       " '12q',\n",
       " '12th',\n",
       " '12v',\n",
       " '12vac',\n",
       " '12vdc',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '130317',\n",
       " '1304',\n",
       " '1305',\n",
       " '130503',\n",
       " '1307',\n",
       " '13070',\n",
       " '130822',\n",
       " '130854',\n",
       " '1309',\n",
       " '130922',\n",
       " '130923',\n",
       " '130c',\n",
       " '130xe',\n",
       " '131',\n",
       " '131239',\n",
       " '1313',\n",
       " '1315',\n",
       " '1316',\n",
       " '131610',\n",
       " '1317',\n",
       " '1318',\n",
       " '131843',\n",
       " '1319',\n",
       " '131900',\n",
       " '131908',\n",
       " '131941',\n",
       " '131954',\n",
       " '132',\n",
       " '1320am',\n",
       " '1321',\n",
       " '132133',\n",
       " '13217',\n",
       " '132429',\n",
       " '1325',\n",
       " '13255',\n",
       " '132604',\n",
       " '1328',\n",
       " '132906',\n",
       " '133',\n",
       " '133130',\n",
       " '1331y',\n",
       " '1333',\n",
       " '1335',\n",
       " '133619',\n",
       " '13370',\n",
       " '1338',\n",
       " '13381',\n",
       " '133866082767180880e',\n",
       " '13389',\n",
       " '133941270127999174e',\n",
       " '134',\n",
       " '1340',\n",
       " '13400',\n",
       " '1341',\n",
       " '1343',\n",
       " '134436',\n",
       " '13445',\n",
       " '134526',\n",
       " '1346',\n",
       " '13467',\n",
       " '1347',\n",
       " '134719io91748',\n",
       " '1348',\n",
       " '134802',\n",
       " '13487',\n",
       " '1349',\n",
       " '13495',\n",
       " '135',\n",
       " '1350',\n",
       " '1354',\n",
       " '1356',\n",
       " '1357',\n",
       " '1358',\n",
       " '135x180',\n",
       " '136',\n",
       " '1360',\n",
       " '13601',\n",
       " '1362',\n",
       " '13622',\n",
       " '136390006',\n",
       " '1366',\n",
       " '1367',\n",
       " '13676',\n",
       " '1368',\n",
       " '1369',\n",
       " '137',\n",
       " '1370',\n",
       " '1371',\n",
       " '1372',\n",
       " '1373',\n",
       " '1374',\n",
       " '1376',\n",
       " '138',\n",
       " '1380',\n",
       " '1383',\n",
       " '1387',\n",
       " '1389',\n",
       " '138p',\n",
       " '139',\n",
       " '139282',\n",
       " '1395',\n",
       " '1396',\n",
       " '1397',\n",
       " '1398',\n",
       " '13apr93',\n",
       " '13e19',\n",
       " '13h',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1401',\n",
       " '1403br',\n",
       " '140401',\n",
       " '140541',\n",
       " '14060',\n",
       " '140649',\n",
       " '1407',\n",
       " '140804',\n",
       " '1409',\n",
       " '140953',\n",
       " '140m',\n",
       " '141',\n",
       " '141034',\n",
       " '1411',\n",
       " '141123',\n",
       " '141137',\n",
       " '1412',\n",
       " '14123',\n",
       " '141557',\n",
       " '141592654',\n",
       " '14179',\n",
       " '1418',\n",
       " '141824',\n",
       " '141842',\n",
       " '142',\n",
       " '1420',\n",
       " '142028',\n",
       " '142037',\n",
       " '1422',\n",
       " '14226',\n",
       " '1424',\n",
       " '142415',\n",
       " '1426',\n",
       " '14261',\n",
       " '142616',\n",
       " '142715',\n",
       " '142747',\n",
       " '1428',\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'amazing',\n",
       " 'bank',\n",
       " 'banks',\n",
       " 'capacitors',\n",
       " 'day',\n",
       " 'gpu',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'lot',\n",
       " 'mars',\n",
       " 'nasa',\n",
       " 'of',\n",
       " 'on',\n",
       " 'performance',\n",
       " 'player',\n",
       " 'rovers',\n",
       " 'shaders',\n",
       " 'shot',\n",
       " 'slap',\n",
       " 'solder',\n",
       " 'soldering',\n",
       " 'spent',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'wicked',\n",
       " 'with',\n",
       " 'yesterday']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_doc_vector = CountVectorizer()\n",
    "fake_doc_counts = fake_doc_vector.fit_transform(fake_docs)\n",
    "fake_doc_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in feature_names\n",
    "'comp.graphics    rec.sport.hockey sci.electronics  sci.space'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics    rec.sport.hockey sci.electronics  sci.space        \n",
      "296              438              240              394              all\n",
      "3                6                4                4                amazing\n",
      "12               1                1                7                bank\n",
      "0                0                1                4                banks\n",
      "0                0                22               0                capacitors\n",
      "28               48               30               75               day\n",
      "69               291              98               201              had\n",
      "273              423              235              360              has\n",
      "664              844              721              760              have\n",
      "76               63               72               68               lot\n",
      "2                0                2                141              mars\n",
      "75               14               68               737              nasa\n",
      "2470             2604             2213             4125             of\n",
      "905              1099             843              1366             on\n",
      "30               5                17               20               performance\n",
      "8                210              24               1                player\n",
      "3                0                0                0                shaders\n",
      "3                94               7                11               shot\n",
      "0                10               0                2                slap\n",
      "0                0                3                0                solder\n",
      "0                0                4                0                soldering\n",
      "8                6                1                30               spent\n",
      "1006             1483             1166             1668             that\n",
      "4537             8080             5527             8264             the\n",
      "2775             3072             2922             3907             to\n",
      "10               27               16               42               today\n",
      "0                0                0                1                wicked\n",
      "705              705              738              753              with\n",
      "5                16               1                11               yesterday\n"
     ]
    }
   ],
   "source": [
    "print(''.join(['{:17}'.format(name) for name in newsgroups.target_names]))\n",
    "for word in fake_doc_vector.get_feature_names():\n",
    "    if word in ('gpu', 'rovers'):\n",
    "        continue\n",
    "    i = word_vector.get_feature_names().index(word)\n",
    "    target_counts = [graphics_counts[i], hockey_counts[i], electronics_counts[i], space_counts[i]]\n",
    "    print(''.join(['{:<17}'.format(count) for count in target_counts])+word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-- \n",
    "- https://stackoverflow.com/questions/5954603/transposing-a-numpy-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now makes sense why gpu doesn't affect the probablities much: it didn't appear in any of the datasets! Also it makes a lot more sense why day and spent had such large biases towards space, as they are so overly represented in that group. The other terms, such as mars and nasa, match up pretty well with my intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
